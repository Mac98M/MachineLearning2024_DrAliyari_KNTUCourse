Welcome to the Wumpus World Project Repository. This project is part of a Machine Learning course where we explore and implement two important reinforcement learning algorithms: Q-learning and Deep Q-Networks (DQN). Our goal is to solve the Wumpus World problem, a classic AI challenge, by training agents to navigate a grid-based environment filled with hazards to find gold and exit safely.

The Wumpus World is a grid-based environment where an agent must navigate to find gold while avoiding deadly pits and a dangerous Wumpus. The environment provides a rich context for testing reinforcement learning algorithms due to its combination of rewards and risks. The agent can move in four directions or shoot arrows to kill the Wumpus, making it a perfect setting for evaluating decision-making algorithms.

In this project, we have implemented and compared two reinforcement learning algorithms: Q-learning and Deep Q-Networks (DQN). Q-learning is a model-free reinforcement learning algorithm that seeks to learn the value of the optimal policy by iteratively updating Q-values based on the agent's experiences. This algorithm uses a Q-table to store and update the value of state-action pairs, gradually converging towards the optimal policy through exploration and exploitation.

Deep Q-Networks (DQN) extend the Q-learning algorithm by utilizing neural networks to approximate Q-values, enabling the algorithm to handle larger and more complex state spaces. DQN addresses the limitations of Q-learning by using experience replay and a target network to stabilize training and improve performance. Experience replay stores the agent's experiences in a buffer and samples from it to break the correlation between consecutive updates. The target network helps to mitigate the issue of moving target values, providing more stable Q-value updates.

We trained the agents using these algorithms and evaluated their performance based on total rewards, cumulative rewards, and mean rewards per episode. The training process involved fine-tuning hyperparameters such as the learning rate, discount factor, exploration rate, and exploration decay rate to optimize the agent's performance. We also visualized the learning curves to analyze the convergence of each algorithm and to understand their strengths and weaknesses in the Wumpus World environment.
