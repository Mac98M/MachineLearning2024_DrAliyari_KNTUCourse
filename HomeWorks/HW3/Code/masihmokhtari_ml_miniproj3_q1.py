# -*- coding: utf-8 -*-
"""masihmokhtari_ml_miniproj3_Q1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_2nGcmVIjCfeN71Mo3PFF4RpK1dQSNWQ

_Machine Learning Dr.Aliyari_


**Masih Mokhtari**

**40211454**
****
_mini project $3$_

$Q1$

## Import Libraries
"""

# At first we disable all warnings.
import warnings
warnings.filterwarnings("ignore")

# Before we start, we have to import libraries we need.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import accuracy_score , confusion_matrix
from sklearn.manifold import TSNE

from sklearn import datasets
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report

import imageio

random_state = 54

"""# Part Zero: Load Dataset"""

# Load the iris dataset from sklearn
iris = datasets.load_iris()

'''iris = load_iris()'''

# Some information about dataset
'''
feature_names':
  [
  'sepal length (cm)',
  'sepal width (cm)',
  'petal length (cm)',
  'petal width (cm)'
  ]


  ('target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10')

# this dataset has 4 features and its target has 3 classes.

#print(iris)
#print(iris.target)
#print("\n")
#print(iris.data)

'''

# Convert the iris dataset to a pandas dataframe
# Add the target variable to the dataframe
'''iris_df['target'] = iris.target'''
iris_df = pd.DataFrame(iris.data, columns = iris.feature_names)
iris_df['target'] = pd.Series(iris.target)

print(f"iris dataframe shape : {iris_df.shape}")
print("\n")

df = iris_df
# Print some information about the dataframe
df.info()
print("\n")
# Print some statistical information about the dataframe
df.describe()

df

df.head()

column = 'target'
num_unique_values = df[column].nunique()
value_counts = df[column].value_counts()
print(f'The number of unique values in the column "{column}" is {num_unique_values}')
print('\n')
print(value_counts)
print('\n')
#y_train.shape
#print(y_train)

"""X and y devide"""

column_names = df.columns
column_names = column_names.to_list()
X = df.iloc[: , :4] # 4 is number of feature columns
y = df['target']

print(f'Dataframe column numbers : {len(column_names)}')

X

_, ax = plt.subplots()
scatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)
ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])
_ = ax.legend(scatter.legend_elements()[0], iris.target_names, loc="lower right", title="Classes")

ax = sns.pairplot(iris_df, hue = 'target')
sns.move_legend(ax, 'lower center', bbox_to_anchor = (.5, 1), ncol = 3, title = 'Pair Plot of IRIS Dataset', frameon = False)
plt.tight_layout()
plt.show()
print("\n")

plt.figure(figsize = (18 , 9))
for i, feature in enumerate(column_names[:-1]):
  plt.subplot(4, 4, i+1)
  sns.histplot(data=iris_df, x=feature, hue='target', kde=True)
  plt.title(f'{feature} Distribution')
plt.tight_layout()
plt.show()

corr = df.corr()
plt.figure(figsize = (10, 8))
sns.heatmap(corr, xticklabels = corr.columns.values, yticklabels = corr.columns.values, cmap = "BuPu", vmin = -1, vmax = 1, annot = True)
plt.title("Correlation Heatmap")
plt.show()

cov_mat = np.cov(X.T)
rounded_matrix_1 = np.round(cov_mat, 3)
print(f'Cvariance Matrix of features is : \n\n {cov_mat}\n\n')
print(f'Cvariance Matrix of features is : \n\n {rounded_matrix_1}\n\n')

eig_val, eig_vec = np.linalg.eig(cov_mat)
exp_var = []

# Sort the eigenvalues in descending order
eig_val = np.sort(eig_val)[::-1]

for i in eig_val:
  var = (i/np.sum(eig_val))*100
  exp_var.append(var)

bar = plt.bar(range(1,5), exp_var, align='center', label='Individual explained variance')

# Add Data labels to the top of bars
for i, bar in enumerate(bar):
  plt.text(bar.get_x()+bar.get_width()/2, bar.get_height(), f'{exp_var[i]:.1f}%', ha='center', va='bottom')


plt.ylabel('Percentage of variance explained (%)')
plt.xlabel('Principle component index')
plt.xticks(ticks = list(range(1,5)))
plt.legend(loc = 'best')
plt.tight_layout()



"""## Preprocess

Train Test Split (Shuffled)
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle=True, random_state = random_state)

X_train

"""Normalization"""

SC = StandardScaler()
X_train_scaled = SC.fit_transform(X_train)
X_test_scaled = SC.transform(X_test)

"""# Part1: Diemnsion Reduction for Visualization purpose

### t_SNE
"""

tsne_model = TSNE(n_components = 2, random_state = random_state)
tsne_Data = tsne_model.fit_transform(X_train)

"""**Plot t-SNE**"""

tsne_df = pd.DataFrame(data = tsne_Data, columns = ['tsne component 1', 'tsne component 2'])
#tsne_df = pd.DataFrame(data = tsne_Data, columns = ['tsne_component_1', 'tsne_component_2', 'tsne_component_3'])
#tsne_df = pd.DataFrame(data = tsne_Data, columns = ['tsne_component_1', 'tsne_component_2', 'tsne_component_3', 'tsne_component_4'])

tsne_df.reset_index(drop = True, inplace= True)
y_train.reset_index(drop = True, inplace= True)

final_tsne_df = tsne_df
final_tsne_df['target'] = y_train.reset_index(drop=True)

ax = sns.scatterplot(x=final_tsne_df.iloc[:,0], y = final_tsne_df.iloc[:,1], hue = 'target', data = final_tsne_df, legend = True)
plt.show()

sns.FacetGrid(final_tsne_df, hue='target', height = 4 ).map(plt.scatter,'tsne component 1', 'tsne component 2')
plt.legend(loc = 'upper right')

final_tsne_df

"""### PCA"""

cov_mat = np.cov(X_train_scaled.T)
rounded_matrix_1 = np.round(cov_mat, 3)
print(f'Cvariance Matrix of features is : \n\n {cov_mat}\n\n')
print(f'Cvariance Matrix of features is : \n\n {rounded_matrix_1}\n\n')

eig_val, eig_vec = np.linalg.eig(cov_mat)
exp_var = []

# Sort the eigenvalues in descending order
eig_val = np.sort(eig_val)[::-1]

for i in eig_val:
  var = (i/np.sum(eig_val))*100
  exp_var.append(var)

bar = plt.bar(range(1,5), exp_var, align='center', label='Individual explained variance')

# Add Data labels to the top of bars
for i, bar in enumerate(bar):
  plt.text(bar.get_x()+bar.get_width()/2, bar.get_height(), f'{exp_var[i]:.1f}%', ha='center', va='bottom')


plt.ylabel('Percentage of variance explained (%)')
plt.xlabel('Principle component index')
plt.xticks(ticks = list(range(1,5)))
plt.legend(loc = 'best')
plt.tight_layout()

pca_model = PCA(n_components = 2 , random_state = random_state)
pca_train = pca_model.fit_transform(X_train_scaled)
pca_test = pca_model.transform(X_test_scaled)

cov_mat_after_FE = np.cov(pca_train.T)
print(f'Cvariance Matrix of features is : \n\n {cov_mat_after_FE}\n\n')

epsilon = 1e-3

# Replace elements smaller than epsilon with zeros
new_cov_matrix = np.where(cov_mat_after_FE < epsilon, 0, cov_mat_after_FE)
rounded_matrix_2 = np.round(cov_mat_after_FE, 3)
# Print the modified matrix
print(new_cov_matrix)
print('\n')
print(f'Covariance Matrix after feature Extraction is :\n{rounded_matrix_2}\n')

"""**Plot PCA (2 components)**"""

pca_df = pd.DataFrame(data= pca_train, columns = ['Principle Component 1', 'Principle Component 2'])
#pca_df = pd.DataFrame(data= pca_train, columns = ['pc 1', 'pc 2', 'pc3'])
pca_df.reset_index(drop = True, inplace = True)
y_train.reset_index(drop = True, inplace = True)
y_train = pd.DataFrame(y_train)
final_pca_df = pca_df
final_pca_df['target'] = y_train.reset_index(drop=True)

ax = sns.scatterplot(x = final_pca_df.iloc[:,0], y = final_pca_df.iloc[:,1], hue = 'target', data = final_pca_df, legend= True)
plt.show()

sns.FacetGrid(final_pca_df, hue='target', height = 5).map(plt.scatter,'Principle Component 1', 'Principle Component 2')
plt.legend(loc = 'upper right')

column = 'target'
num_unique_values = final_pca_df[column].nunique()
value_counts = final_pca_df[column].value_counts()
print(f'The number of unique values in the column "{column}" is {num_unique_values}')
print('\n')
print(value_counts)
print('\n')

final_pca_df

"""### LDA"""

lda_model = LinearDiscriminantAnalysis(n_components=2)  # , random_state = random_state
X_train_lda = lda_model.fit_transform(X_train_scaled, y_train)
X_test_lda = lda_model.transform(X_test_scaled)

import numpy as np
import pandas as pd

# Check for NaN values in transformed data
print("NaN in X_train_lda:", np.isnan(X_train_lda).sum())
print("NaN in X_test_lda:", np.isnan(X_test_lda).sum())
print('\n')

# Check shapes of transformed data and target variable
print("Shape of X_train_lda:", X_train_lda.shape)
print("Shape of y_train:", y_train.shape)
print('\n')

# Check for NaN values in the original scaled data
print("NaN in X_train_scaled:", np.isnan(X_train_scaled).sum())
print('\n')

# Check for NaN values in y_train
print("NaN in y_train:", np.isnan(y_train).sum())

cov_mat_after_FE_2 = np.cov(X_train_lda.T)
print(f'Cvariance Matrix of features is : \n\n {cov_mat_after_FE_2}\n\n')

epsilon = 1e-3

# Replace elements smaller than epsilon with zeros
new_cov_matrix_2 = np.where(cov_mat_after_FE_2 < epsilon, 0, cov_mat_after_FE_2)
rounded_matrix_3 = np.round(cov_mat_after_FE_2, 3)
# Print the modified matrix
print(new_cov_matrix_2)
print('\n')
print(f'Covariance Matrix after feature Extraction is :\n{rounded_matrix_3}\n')

# Create the DataFrame with LDA components
lda_df = pd.DataFrame(X_train_lda, columns=['LDA Component 1', 'LDA Component 2'])
lda_df['target'] = y_train.reset_index(drop=True) # Ensure y_train is correctly aligned and has no NaN values



# Display the DataFrame to check for any issues
print(lda_df.head())
print('\n')
print(lda_df.info())
print('\n')
print(lda_df.isna().sum())


# Check for dtype issues
print(lda_df.dtypes)
print(y_train.dtypes)
print('\n')

ax = sns.scatterplot(x = lda_df.iloc[:,0], y = lda_df.iloc[:,1], hue = 'target', data = lda_df, legend= True)
plt.show()

'''
lda_df = pd.DataFrame(X_train_lda, columns=['LDA Component 1', 'LDA Component 2'])
lda_df['target'] =y_train
'''
sns.FacetGrid(lda_df, hue='target', height = 5).map(plt.scatter,'LDA Component 1', 'LDA Component 2')
plt.legend(loc = 'upper right')

column = 'target'
num_unique_values = lda_df[column].nunique()
value_counts = lda_df[column].value_counts()
print(f'The number of unique values in the column "{column}" is {num_unique_values}')
print('\n')
print(value_counts)
print('\n')
#y_train.shape
#print(y_train)

lda_df

"""# Part2: Linear SVM Classifier

## Without Feature Extraction
"""

X_svc_trn = X_train_scaled
X_svc_tst = X_test_scaled
y_svc_trn = y_train
y_svc_tst = y_test

# Assuming X_svc_trn is a numpy array, if it's a DataFrame, convert it to numpy array
X_svc_trn = X_svc_trn.to_numpy() if isinstance(X_svc_trn, pd.DataFrame) else X_svc_trn
X_svc_tst = X_svc_tst.to_numpy() if isinstance(X_svc_tst, pd.DataFrame) else X_svc_tst
y_svc_trn = y_svc_trn.to_numpy() if isinstance(y_svc_trn, pd.DataFrame) else y_svc_trn

clf_without_DR = SVC(kernel = 'linear', C = 0.5, random_state=random_state)
clf_without_DR.fit(X_svc_trn, y_svc_trn)

print(f'Support Vectors are :\n {clf_without_DR.support_vectors_}')
print(f'\nSupport Vectors of each classes are : \n{clf_without_DR.n_support_}\n')

print('weights : \n', clf_without_DR.coef_, '\n\nbias : \n', clf_without_DR.intercept_)

y_pred_without_DR = clf_without_DR.predict(X_svc_tst)
cm = confusion_matrix(y_svc_tst, y_pred_without_DR)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix for Linear Kernel SVM")
plt.show()

SVM_score_without_DR = clf_without_DR.score(X_svc_tst, y_svc_tst) # SVM performance score without Dimension Reduction
print(f'\t\tSVM performance score without Dimension Reduction\n\t\tAccuracy : {SVM_score_without_DR :.2f} %')

print("\nClassification Report: \n")
print(f'{classification_report(y_svc_tst,y_pred_without_DR)}')

"""
## With Feature Extraction"""

# Visualization_1
def make_meshgrid(x, y, h=.02):
    x_min, x_max = x.min() - 1, x.max() + 1
    y_min, y_max = y.min() - 1, y.max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    return xx, yy

def plot_contours(ax, clf, xx, yy, **params):
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    out = ax.contourf(xx, yy, Z, **params)
    return out

# Visualization_2
def plot_decision_boundary(svm_model, X, y, title):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                         np.arange(y_min, y_max, 0.01))

    Z = svm_model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(5, 5))
    plt.contourf(xx, yy, Z, alpha=0.1)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')
    plt.title(title)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    #plt.grid()
    plt.show()

"""### SVM PCA"""

X_trn_sc_pca = final_pca_df.drop('target', axis=1).values
y_trn_sc_pca = final_pca_df['target'].values

X_svc_trn = X_trn_sc_pca
X_svc_tst = pca_test
y_svc_trn = y_trn_sc_pca
y_svc_tst = y_test

clf_pca = SVC(kernel = 'linear', C=0.5, random_state=random_state)
clf_pca.fit(X_svc_trn, y_svc_trn)

clf = clf_pca

print(f'Support Vectors are :\n {clf.support_vectors_}')
print(f'\nSupport Vectors of each classes are : \n{clf.n_support_}\n')
print('weights : \n', clf.coef_, '\n\nbias : \n', clf.intercept_)

# Train part
SVM_score_pca = clf.score(X_svc_trn, y_svc_trn) # SVM performance score without Dimension Reduction
print(f'\n\t\tSVM performance score Train set\n\t\tAccuracy : {SVM_score_pca :.2f} %')
plot_decision_boundary(clf, X_svc_trn, y_svc_trn, 'Linear SVM (PCA and Train set)')

# Test part
y_pred_pca = clf.predict(X_svc_tst)
cm = confusion_matrix(y_svc_tst, y_pred_pca)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix for Linear Kernel SVM")
plt.show()

SVM_score_pca = clf.score(X_svc_tst, y_svc_tst) # SVM performance score without Dimension Reduction
print(f'\t\tSVM performance score Test set\n\t\tAccuracy : {SVM_score_pca :.2f} %')

print("\nClassification Report: \n")
print(f'{classification_report(y_svc_tst,y_pred_pca)}')

plot_decision_boundary(clf, X_svc_tst, y_svc_tst, 'Linear SVM (PCA and Test set)')

# Visualization_1
clf = clf_pca
# Create a new figure and axis
fig, ax = plt.subplots()

# Assuming X_svc_trn is a numpy array, if it's a DataFrame, convert it to numpy array
X_svc_trn_array = X_svc_trn.to_numpy() if isinstance(X_svc_trn, pd.DataFrame) else X_svc_trn

# Extract the first and second columns from X_svc_trn_array
X0, X1 = X_svc_trn_array[:, 0], X_svc_trn_array[:, 1]

# Create meshgrid
xx, yy = make_meshgrid(X0, X1)

#clf = SVC(kernel='linear', C=0.5, random_state=random_state)

# Plot contours
plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)

# Scatter plot of training points
ax.scatter(X0, X1, c=y_svc_trn, cmap=plt.cm.coolwarm, s=20, edgecolors='k')
ax.set_xlim(xx.min(), xx.max())
ax.set_ylim(yy.min(), yy.max())
ax.set_xlabel('Feature 1')
ax.set_ylabel('Feature 2')
ax.set_xticks(())
ax.set_yticks(())
ax.set_title('SVM on IRIS Dataset')

# Show the plot
plt.show()

# Visualization_2
clf = clf_pca
plot_decision_boundary(clf, X_svc_tst, y_svc_tst, 'Linear SVM (PCA and Test set)')

"""### SVM LDA"""

X_trn_sc_lda = lda_df.drop('target', axis=1).values
y_trn_sc_lda = lda_df['target'].values

X_svc_trn = X_trn_sc_lda
X_svc_tst = X_test_lda
y_svc_trn = y_trn_sc_lda
y_svc_tst = y_test

clf_lda = SVC(kernel = 'linear', C=0.5, random_state=random_state)
clf_lda.fit(X_svc_trn, y_svc_trn)

clf = clf_lda

print(f'Support Vectors are :\n {clf.support_vectors_}')
print(f'\nSupport Vectors of each classes are : \n{clf.n_support_}\n')
print('weights : \n', clf.coef_, '\n\nbias : \n', clf.intercept_)

# Train part
SVM_score_lda = clf.score(X_svc_trn, y_svc_trn) # SVM performance score without Dimension Reduction
print(f'\n\t\tSVM performance score Train set\n\t\tAccuracy : {SVM_score_lda :.2f} %')
plot_decision_boundary(clf, X_svc_trn, y_svc_trn, 'Linear SVM (LDA and Train set)')

# Test part
y_pred_lda = clf.predict(X_svc_tst)
cm = confusion_matrix(y_svc_tst, y_pred_lda)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix for Linear Kernel SVM")
plt.show()

SVM_score_lda = clf.score(X_svc_tst, y_svc_tst) # SVM performance score without Dimension Reduction
print(f'\t\tSVM performance score Test set\n\t\tAccuracy : {SVM_score_lda :.2f} %')

print("\nClassification Report: \n")
print(f'{classification_report(y_svc_tst,y_pred_lda)}')

plot_decision_boundary(clf, X_svc_tst, y_svc_tst, 'Linear SVM (LDA and Test set)')

"""# Part3: SVM Classifier with Polynomail Kernal

## A_poly_deg3_LDA
"""

X_trn_sc_lda = lda_df.drop('target', axis=1).values
y_trn_sc_lda = lda_df['target'].values

X_svc_trn = X_trn_sc_lda
X_svc_tst = X_test_lda
y_svc_trn = y_trn_sc_lda
y_svc_tst = y_test

degree = 3

clf_lda = SVC(kernel = 'poly', C=0.5, degree=degree, random_state=random_state)
clf_lda.fit(X_svc_trn, y_svc_trn)

clf = clf_lda

print(f'Support Vectors are :\n {clf.support_vectors_}')
print(f'\nSupport Vectors of each classes are : \n{clf.n_support_}\n')
#print('weights : \n', clf.coef_, '\n\nbias : \n', clf.intercept_)

# Train part
SVM_score_lda = clf.score(X_svc_trn, y_svc_trn) # SVM performance score without Dimension Reduction
print(f'\n\t\tSVM performance score Train set\n\t\tAccuracy : {SVM_score_lda :.2f} %')
plot_decision_boundary(clf, X_svc_trn, y_svc_trn, 'SVM (Degree{3} LDA and Train set)')

# Test part
y_pred_lda = clf.predict(X_svc_tst)
cm = confusion_matrix(y_svc_tst, y_pred_lda)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix for SVM")
plt.show()

SVM_score_lda = clf.score(X_svc_tst, y_svc_tst) # SVM performance score without Dimension Reduction
print(f'\t\tSVM performance score Test set\n\t\tAccuracy : {SVM_score_lda :.2f} %')

print("\nClassification Report: \n")
print(f'{classification_report(y_svc_tst,y_pred_lda)}')

plot_decision_boundary(clf, X_svc_tst, y_svc_tst, 'SVM (Degree{3} LDA and Test set)')

"""## B_poly_deg3_PCA"""

X_trn_sc_pca = final_pca_df.drop('target', axis=1).values
y_trn_sc_pca = final_pca_df['target'].values

X_svc_trn = X_trn_sc_pca
X_svc_tst = pca_test
y_svc_trn = y_trn_sc_pca
y_svc_tst = y_test

clf_pca = SVC(kernel = 'poly', C=0.5, degree=degree, random_state=random_state)
clf_pca.fit(X_svc_trn, y_svc_trn)

clf = clf_pca

print(f'Support Vectors are :\n {clf.support_vectors_}')
print(f'\nSupport Vectors of each classes are : \n{clf.n_support_}\n')
#print('weights : \n', clf.coef_, '\n\nbias : \n', clf.intercept_)

# Train part
SVM_score_pca = clf.score(X_svc_trn, y_svc_trn) # SVM performance score without Dimension Reduction
print(f'\n\t\tSVM performance score Train set\n\t\tAccuracy : {SVM_score_pca :.2f} %')
plot_decision_boundary(clf, X_svc_trn, y_svc_trn, 'SVM (Degree{3} PCA and Train set)')

# Test part
y_pred_pca = clf.predict(X_svc_tst)
cm = confusion_matrix(y_svc_tst, y_pred_pca)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix for SVM")
plt.show()

SVM_score_pca = clf.score(X_svc_tst, y_svc_tst) # SVM performance score without Dimension Reduction
print(f'\t\tSVM performance score Test set\n\t\tAccuracy : {SVM_score_pca :.2f} %')

print("\nClassification Report: \n")
print(f'{classification_report(y_svc_tst,y_pred_pca)}')

plot_decision_boundary(clf, X_svc_tst, y_svc_tst, 'SVM (Degree{3} PCA and Test set)')

"""## C_poly_task"""

def plot_decision_boundary(clf, X, y, title , save_path):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))

    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(5, 5))
    plt.contourf(xx, yy, Z, alpha=0.5)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')
    plt.title(title)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.savefig(save_path)
    plt.show()

X_trn_sc_lda = lda_df.drop('target', axis=1).values
y_trn_sc_lda = lda_df['target'].values

X_svc_trn = X_trn_sc_lda
X_svc_tst = X_test_lda
y_svc_trn = y_trn_sc_lda
y_svc_tst = y_test

results = []
accuracy_Train = []
accuracy_Test = []

accuracy_Train_ = []
accuracy_Test = []

for degree in range(1, 11):
    svm_poly = SVC(kernel = 'poly', C=0.5, degree=degree, random_state=random_state)
    svm_poly.fit(X_svc_trn, y_svc_trn)

    clf = svm_poly

    # Train part
    SVM_score_lda_train = clf.score(X_svc_trn, y_svc_trn) # SVM performance score without Dimension Reduction
    print(f'\n\t\tSVM performance score Train set\n\t\tAccuracy : {SVM_score_lda_train :.2f} %')
    plot_decision_boundary(clf, X_svc_trn, y_svc_trn, f'SVM (Degree{degree} LDA and Train set)', save_path = f'svm_poly_degree_{degree}_Train.png')
    accuracy_Train.append(SVM_score_lda_train)

    # Test part
    y_pred_lda = clf.predict(X_svc_tst)
    cm = confusion_matrix(y_svc_tst, y_pred_lda)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)
    disp.plot(cmap=plt.cm.Blues)
    plt.title("Confusion Matrix for SVM")
    plt.show()

    SVM_score_lda_test = clf.score(X_svc_tst, y_svc_tst) # SVM performance score without Dimension Reduction
    print(f'\t\tSVM performance score Test set\n\t\tAccuracy : {SVM_score_lda_test :.2f} %')
    accuracy_Test.append(SVM_score_lda_test)

    print("\nClassification Report: \n")
    print(f'{classification_report(y_svc_tst,y_pred_lda)}')

    plt.figure()
    plot_decision_boundary(clf, X_svc_tst, y_svc_tst, f'SVM (Degree{degree} LDA and Test set)', save_path = f'svm_poly_degree_{degree}_Test.png')

    results.append((degree, SVM_score_lda_train, SVM_score_lda_test))

accuracy_Train_lib = accuracy_Train.copy()
accuracy_Test_lib = accuracy_Test.copy()

images_Train = []
images_Test = []
# Creating a list of durations (in seconds) for each frame
#durations = [1] * 50  # 1 second delay after each frame
durations = []

for degree in range(1, 11):
    images_Train.append(imageio.imread(f'svm_poly_degree_{degree}_Train.png'))
    durations.append(1)  # 1 second delay for each image
imageio.mimsave('svm_poly_kernels_Train.gif', images_Train, duration=durations)


for degree in range(1, 11):
    images_Test.append(imageio.imread(f'svm_poly_degree_{degree}_Test.png'))
imageio.mimsave('svm_poly_kernels_Test.gif', images_Test, duration=durations)

from PIL import Image, ImageSequence

images_Train = []
images_Test = []

for degree in range(1, 11):
    images_Train.append(Image.open(f'svm_poly_degree_{degree}_Train.png'))

for degree in range(1, 11):
    images_Test.append(Image.open(f'svm_poly_degree_{degree}_Test.png'))

# Create a new GIF with a duration for each frame
images_Train[0].save('svm_poly_kernels_Train.gif', save_all=True, append_images=images_Train[1:], duration=1000, loop=0)
images_Test[0].save('svm_poly_kernels_Test.gif', save_all=True, append_images=images_Test[1:], duration=1000, loop=0)

# The duration is set in milliseconds, so 1000 ms = 1 second

print(f'\tDegree','\t\t', f'| \tAccuracy_Train\t', f'| \tAccuracy_Test')
print(f'_________________________|_______________________|___________________________')
for degree, acc_trn, acc_tst in results:

    #print(f'Degree {degree}','\t', f'| \tAccuracy_Train = {acc_trn:.2f}', f'| \tAccuracy_Test = {acc_tst:.2f}')
    print(f'\t  {degree}','\t\t', f'| \t     {acc_trn:.2f}', f'\t |\t    {acc_tst:.2f}')

# Assuming accuracy_Train and accuracy_Test are already defined
degrees = range(1, len(accuracy_Train) + 1)

plt.figure(figsize=(8, 5))
plt.plot(degrees, accuracy_Train, 'r-o', label='Train Accuracy')
plt.plot(degrees, accuracy_Test, 'g-o', label='Test Accuracy')
plt.title('Accuracy')
plt.xlabel('Degree')
plt.legend()
plt.grid()

# Set the x-axis to start from 1
plt.xticks(degrees)

plt.show()

"""# Part4: SVM Classifier with Polynomail Kernal_From Scratch"""

def plot_decision_boundary(clf, X, y, title , save_path):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))

    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(5, 5))
    plt.contourf(xx, yy, Z, alpha=0.5)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')
    plt.title(title)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.savefig(save_path)
    plt.show()

import numpy as np

class SVM:
    def __init__(self, degree=3, C=1.0, tol=1e-3, max_passes=5):
        self.degree = degree
        self.C = C
        self.tol = tol
        self.max_passes = max_passes
        self.models = []

    def polynomial_kernel(self, x, y, degree, c=1):
        return (np.dot(x, y) + c) ** degree

    def fit(self, X, y):
        self.classes = np.unique(y)
        self.models = []

        for cls in self.classes:
            y_binary = np.where(y == cls, 1, -1)
            model = self._train_binary_classifier(X, y_binary)
            self.models.append(model)

    def _train_binary_classifier(self, X, y):
        n_samples, n_features = X.shape
        alpha = np.zeros(n_samples)
        b = 0
        passes = 0

        while passes < self.max_passes:
            alpha_prev = np.copy(alpha)
            for i in range(n_samples):
                E_i = self._decision_function(X[i], X, y, alpha, b) - y[i]

                if (y[i] * E_i < -self.tol and alpha[i] < self.C) or (y[i] * E_i > self.tol and alpha[i] > 0):
                    j = np.random.choice([x for x in range(n_samples) if x != i])
                    E_j = self._decision_function(X[j], X, y, alpha, b) - y[j]

                    alpha_i_old, alpha_j_old = alpha[i], alpha[j]

                    if y[i] != y[j]:
                        L = max(0, alpha[j] - alpha[i])
                        H = min(self.C, self.C + alpha[j] - alpha[i])
                    else:
                        L = max(0, alpha[j] + alpha[i] - self.C)
                        H = min(self.C, alpha[j] + alpha[i])

                    if L == H:
                        continue

                    eta = 2 * self.polynomial_kernel(X[i], X[j], self.degree) - self.polynomial_kernel(X[i], X[i], self.degree) - self.polynomial_kernel(X[j], X[j], self.degree)
                    if eta >= 0:
                        continue

                    alpha[j] -= y[j] * (E_i - E_j) / eta
                    alpha[j] = np.clip(alpha[j], L, H)

                    if abs(alpha[j] - alpha_j_old) < 1e-5:
                        continue

                    alpha[i] += y[i] * y[j] * (alpha_j_old - alpha[j])

                    b1 = b - E_i - y[i] * (alpha[i] - alpha_i_old) * self.polynomial_kernel(X[i], X[i], self.degree) - y[j] * (alpha[j] - alpha_j_old) * self.polynomial_kernel(X[i], X[j], self.degree)
                    b2 = b - E_j - y[i] * (alpha[i] - alpha_i_old) * self.polynomial_kernel(X[i], X[j], self.degree) - y[j] * (alpha[j] - alpha_j_old) * self.polynomial_kernel(X[j], X[j], self.degree)

                    if 0 < alpha[i] < self.C:
                        b = b1
                    elif 0 < alpha[j] < self.C:
                        b = b2
                    else:
                        b = (b1 + b2) / 2

            diff = np.linalg.norm(alpha - alpha_prev)
            if diff < self.tol:
                passes += 1
            else:
                passes = 0

        return {'alpha': alpha, 'b': b, 'X': X, 'y': y}

    def _decision_function(self, x, X, y, alpha, b):
        return np.sum([alpha[i] * y[i] * self.polynomial_kernel(x, X[i], self.degree) for i in range(len(alpha))]) + b

    def predict(self, X):
        predictions = np.zeros((X.shape[0], len(self.models)))

        for i, model in enumerate(self.models):
            predictions[:, i] = [self._decision_function(x, model['X'], model['y'], model['alpha'], model['b']) for x in X]

        return self.classes[np.argmax(predictions, axis=1)]

if __name__ == "__main__":
  import os
  import numpy as np
  import pandas as pd
  from sklearn.model_selection import train_test_split
  from sklearn.svm import SVC
  from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
  from sklearn.decomposition import PCA
  import matplotlib.pyplot as plt
  import seaborn as sns
  import imageio

degree = 5
svm_poly = SVM(C=0.5, degree=degree)
clf = svm_poly
clf.fit(X_svc_trn, y_svc_trn)
y_pred_train = clf.predict(X_svc_trn)
SVM_score_lda_train = accuracy_score(y_train, y_pred_train)
print(f'\n\t\tSVM performance score Train set\n\t\tAccuracy : {SVM_score_lda_train :.2f} %')
plot_decision_boundary(clf, X_svc_trn, y_svc_trn, f'SVM (Degree{degree} LDA and Train set)', save_path = f'svm_poly_degree_{degree}_Train.png')

X_trn_sc_lda = lda_df.drop('target', axis=1).values
y_trn_sc_lda = lda_df['target'].values

X_svc_trn = X_trn_sc_lda
X_svc_tst = X_test_lda
y_svc_trn = y_trn_sc_lda
y_svc_tst = y_test

results = []
accuracy_Train = []
accuracy_Test = []


for degree in range(1, 11):
    #svm_poly = SVC(kernel = 'poly', C=0.5, degree=degree, random_state=random_state)
    svm_poly = SVM(C=1.0, degree= degree)
    svm_poly.fit(X_svc_trn, y_svc_trn)

    clf = svm_poly

    # Train part
    #SVM_score_lda_train = clf.score(X_svc_trn, y_svc_trn) # SVM performance score without Dimension Reduction
    y_pred_train = clf.predict(X_svc_trn)
    SVM_score_lda_train = accuracy_score(y_svc_trn, y_pred_train)
    print(f'\n\t\tSVM performance score Train set\n\t\tAccuracy : {SVM_score_lda_train :.2f} %')
    plot_decision_boundary(clf, X_svc_trn, y_svc_trn, f'SVM (Degree{degree} LDA and Train set)', save_path = f'svm_poly_degree_{degree}_Train.png')
    accuracy_Train.append(SVM_score_lda_train)

    # Test part
    y_pred_lda = clf.predict(X_svc_tst)
    cm = confusion_matrix(y_svc_tst, y_pred_lda)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)
    disp.plot(cmap=plt.cm.Blues)
    plt.title("Confusion Matrix for SVM")
    plt.show()

    #SVM_score_lda_test = clf.score(X_svc_tst, y_svc_tst) # SVM performance score without Dimension Reduction
    SVM_score_lda_test = accuracy_score(y_svc_tst, y_pred_lda)
    print(f'\t\tSVM performance score Test set\n\t\tAccuracy : {SVM_score_lda_test :.2f} %')
    accuracy_Test.append(SVM_score_lda_test)

    print("\nClassification Report: \n")
    print(f'{classification_report(y_svc_tst,y_pred_lda)}')

    plt.figure()
    plot_decision_boundary(clf, X_svc_tst, y_svc_tst, f'SVM (Degree{degree} LDA and Test set)', save_path = f'svm_poly_degree_{degree}_Test.png')

    results.append((degree, SVM_score_lda_train, SVM_score_lda_test))

accuracy_Train_fromscratch = accuracy_Train.copy()
accuracy_Test_fromscratch = accuracy_Test.copy()

images_Train = []
images_Test = []
# Creating a list of durations (in seconds) for each frame
#durations = [1] * 50  # 1 second delay after each frame
durations = []

for degree in range(1, 11):
    images_Train.append(imageio.imread(f'svm_poly_degree_{degree}_Train.png'))
    durations.append(1)  # 1 second delay for each image
imageio.mimsave('svm_poly_kernels_Train.gif', images_Train, duration=durations)


for degree in range(1, 11):
    images_Test.append(imageio.imread(f'svm_poly_degree_{degree}_Test.png'))
imageio.mimsave('svm_poly_kernels_Test.gif', images_Test, duration=durations)

from PIL import Image, ImageSequence

images_Train = []
images_Test = []

for degree in range(1, 11):
    images_Train.append(Image.open(f'svm_poly_degree_{degree}_Train.png'))

for degree in range(1, 11):
    images_Test.append(Image.open(f'svm_poly_degree_{degree}_Test.png'))

# Create a new GIF with a duration for each frame
images_Train[0].save('svm_poly_kernels_Train.gif', save_all=True, append_images=images_Train[1:], duration=1000, loop=0)
images_Test[0].save('svm_poly_kernels_Test.gif', save_all=True, append_images=images_Test[1:], duration=1000, loop=0)

# The duration is set in milliseconds, so 1000 ms = 1 second

print(f'\tDegree','\t\t', f'| \tAccuracy_Train\t', f'| \tAccuracy_Test')
print(f'_________________________|_______________________|___________________________')
for degree, acc_trn, acc_tst in results:

    #print(f'Degree {degree}','\t', f'| \tAccuracy_Train = {acc_trn:.2f}', f'| \tAccuracy_Test = {acc_tst:.2f}')
    print(f'\t  {degree}','\t\t', f'| \t     {acc_trn:.2f}', f'\t |\t    {acc_tst:.2f}')

# Assuming accuracy_Train and accuracy_Test are already defined
degrees = range(1, len(accuracy_Train) + 1)

plt.figure(figsize=(8, 5))
plt.plot(degrees, accuracy_Train, 'r-o', label='Train Accuracy')
plt.plot(degrees, accuracy_Test, 'g-o', label='Test Accuracy')
plt.title('Accuracy')
plt.xlabel('Degree')
plt.legend()
plt.grid()

# Set the x-axis to start from 1
plt.xticks(degrees)

plt.show()

# Assuming accuracy_Train and accuracy_Test are already defined
degrees = range(1, len(accuracy_Train) + 1)

plt.figure(figsize=(8, 5))
plt.plot(degrees, accuracy_Test_lib, 'r-o', label='Test Accuracy_Library')
plt.plot(degrees, accuracy_Test_fromscratch, 'g-o', label='Test Accuracy_From Scratch')
plt.title('Accuracy')
plt.xlabel('Degree')
plt.legend()
plt.grid()

# Set the x-axis to start from 1
plt.xticks(degrees)

plt.show()

"""# Not important"""

def make_meshgrid(x, y, h=.02):
    x_min, x_max = x.min() - 1, x.max() + 1
    y_min, y_max = y.min() - 1, y.max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    return xx, yy

def plot_contours(ax, clf, xx, yy, **params):
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    out = ax.contourf(xx, yy, Z, **params)
    return out


classifiers = [
    SVC(kernel='linear', C=0.5, random_state=random_state),
    SVC(kernel='linear', C=1, random_state=random_state),
    SVC(kernel='rbf', gamma='auto', C=0.5)
]

# Titles for the plots
titles = ['SVC with linear kernel',
          'SVC with linear kernel2',
          'SVC with RBF kernel'
]

fig, sub = plt.subplots(3, 1)
plt.subplots_adjust(wspace=0.5, hspace=0.5)

X0, X1 = X_svc_trn[:, 0], X_svc_trn[:, 1]
xx, yy = make_meshgrid(X0, X1)

for clf, title, ax in zip(classifiers, titles, sub.flatten()):
    clf.fit(X_svc_trn, y_svc_trn)
    plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)
    ax.scatter(X0, X1, c=y_svc_trn, cmap=plt.cm.coolwarm, s=20, edgecolors='k')
    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xlabel('Feature 1')
    ax.set_ylabel('Feature 2')
    ax.set_xticks(())
    ax.set_yticks(())
    ax.set_title(title)

plt.show()

"""
from scratch_Method2

"""

import cvxopt
def linear_kernel( x1, x2):
    return np.dot(x1, x2)

def polynomial_kernel( x, y, C=1.0, d=3):
    return (np.dot(x, y) + C) ** d

def gaussian_kernel( x, y, gamma=0.5):
    return np.exp(-gamma*np.linalg.norm(x - y) ** 2)

def sigmoid_kernel( x, y, alpha=1, C=0.01):
    a= alpha * np.dot(x, y) + C
    return np.tanh(a)

def SVM_scratch(X, X_t, y, C, kernel_type, poly_params=(1, 4), RBF_params=0.5, sigmoid_params=(1, 0.01)):
    kernel_and_params=(kernel_type,poly_params, RBF_params, sigmoid_params,C)
    n_samples, n_features = X.shape
    # Compute the Gram matrix
    K = np.zeros((n_samples, n_samples))
    if kernel_type == 'linear':
        for i in range(n_samples):
            for j in range(n_samples):
                K[i, j] = linear_kernel(X[i], X[j])
    elif kernel_type == 'polynomial':
        for i in range(n_samples):
            for j in range(n_samples):
                K[i, j] = polynomial_kernel(X[i], X[j], poly_params[0], poly_params[1])
    elif kernel_type == 'RBF':
        for i in range(n_samples):
            for j in range(n_samples):
                K[i, j] = gaussian_kernel(X[i], X[j], RBF_params)
    elif kernel_type == 'sigmoid':
        for i in range(n_samples):
            for j in range(n_samples):
                K[i, j] = sigmoid_kernel(X[i], X[j], sigmoid_params[0], sigmoid_params[1])
    else:
        raise ValueError("Invalid kernel type")

    # construct P, q, A, b, G, h matrices for CVXOPT
    P = cvxopt.matrix(np.outer(y, y) * K)
    q = cvxopt.matrix(np.ones(n_samples) * -1)
    A = cvxopt.matrix(y, (1, n_samples))
    b = cvxopt.matrix(0.0)
    G = cvxopt.matrix(np.vstack((np.diag(np.ones(n_samples) * -1), np.identity(n_samples))))
    h = cvxopt.matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * C)))
    # solve QP problem
    cvxopt.solvers.options['show_progress'] = False
    solution = cvxopt.solvers.qp(P, q, G, h, A, b)
    # Lagrange multipliers
    a = np.ravel(solution['x'])
    # Support vectors have non zero lagrange multipliers
    sv = a > 1e-5  # some small threshold

    ind = np.arange(len(a))[sv]
    a = a[sv]
    sv_x = X[sv]
    sv_y = y[sv]
    numbers_of_sv=len(sv_y)
    # Bias (For linear it is the intercept):
    bias = 0
    for n in range(len(a)):
        # For all support vectors:
        bias += sv_y[n]
        bias -= np.sum(a * sv_y * K[ind[n], sv])
    bias = bias / (len(a)+0.0001)

    if kernel_type == 'linear':
        w = np.zeros(n_features)
        for n in range(len(a)):
            w += a[n] * sv_y[n] * sv_x[n]
    else:
        w = None

    y_pred=0
    if w is not None:
        y_pred = np.sign(np.dot(X_t, w) + bias)
    else:
        y_predict = np.zeros(len(X_t))
        for i in range(len(X_t)):
            s = 0
            for a1, sv_y1, sv1 in zip(a ,sv_y, sv_x):
                # a : Lagrange multipliers, sv : support vectors.
                # Hypothesis: sign(sum^S a * y * kernel + b)

                if kernel_type == 'linear':
                    s += a1 * sv_y1 * linear_kernel(X_t[i], sv1)
                if kernel_type=='RBF':
                    s += a1 * sv_y1 * gaussian_kernel(X_t[i], sv1, RBF_params)   # Kernel trick.
                if kernel_type == 'polynomial':
                    s += a1 * sv_y1 * polynomial_kernel(X_t[i], sv1, poly_params[0], poly_params[1])
                if kernel_type == 'sigmoid':
                    s=+ a1 * sv_y1 *sigmoid_kernel( X_t[i],  sv1, sigmoid_params[0], sigmoid_params[1])
            y_predict[i] = s
        y_pred = np.sign(y_predict + bias)

    return w, bias, solution,a, sv_x, sv_y, y_pred, kernel_and_params



def multiclass_svm(X, X_t, y, C, kernel_type, poly_params=(1, 4), RBF_params=0.5, sigmoid_params=(1, 0.01)):
    class_labels = list(set(y))

    classifiers = {}
    w_catch = {}  # catching w, b only for plot part
    b_catch = {}
    a_catch = {}
    sv_x_catch = {}
    sv_y_catch = {}

    for i, class_label in enumerate(class_labels):
        binary_y = np.where(y == class_label, 1.0, -1.0)
        w, bias, solution, a, sv_x, sv_y, prediction, kernel_and_params = SVM_scratch(X, X_t, binary_y, C, kernel_type, poly_params, RBF_params, sigmoid_params)
        classifiers[class_label] = (w, bias, a, sv_x, sv_y, kernel_and_params)
        w_catch[class_label] = w
        b_catch[class_label] = bias
        a_catch[class_label] = a
        sv_x_catch[class_label] = sv_x
        sv_y_catch[class_label] = sv_y

    def decision_function(X_t):
        decision_scores = np.zeros((X_t.shape[0], len(class_labels)))
        for i, label in enumerate(class_labels):
            w, bias, a, sv_x, sv_y, kernel_and_params = classifiers[label]
            if w is not None:
                decision_scores[:, i] = np.dot(X_t, w) + bias
            else:
                decision_values = np.zeros(X_t.shape[0])
                for j in range(X_t.shape[0]):
                    s = 0
                    for a1, sv_y1, sv1 in zip(a, sv_y, sv_x):
                        if kernel_type == 'linear':
                            s += a1 * sv_y1 * linear_kernel(X_t[j], sv1)
                        elif kernel_type == 'RBF':
                            s += a1 * sv_y1 * gaussian_kernel(X_t[j], sv1, RBF_params)
                        elif kernel_type == 'polynomial':
                            s += a1 * sv_y1 * polynomial_kernel(X_t[j], sv1, poly_params[0], poly_params[1])
                        elif kernel_type == 'sigmoid':
                            s += a1 * sv_y1 * sigmoid_kernel(X_t[j], sv1, sigmoid_params[0], sigmoid_params[1])
                    decision_values[j] = s
                decision_scores[:, i] = decision_values + bias
        return np.argmax(decision_scores, axis=1), kernel_and_params, w_catch, b_catch, classifiers

    return decision_function(X_t)

def visualize_multiclass_classification1(X_train, y_train1, kernel_type, trainset, classifiers, class_labels, w_stack, b_stack,save_path_scr, epsilon=1e-10):
    plt.figure(figsize=(6, 4))
    for i, target_name in enumerate(class_labels):
        plt.scatter(X_train[y_train1 == i, 0], X_train[y_train1 == i, 1], label=target_name)

    if kernel_type == 'linear':
        for i in range(len(class_labels)):
            w = w_stack[i]
            bias = b_stack[i]
            x_points = np.linspace(np.min(X_train[:, 0]) - 1, np.max(X_train[:, 0]) + 1, 200)
            y_points = -(w[0] / (w[1] + epsilon)) * x_points - bias / (w[1] + epsilon)
            plt.plot(x_points, y_points, c='r', label='Decision Boundary')

    elif kernel_type == 'polynomial':
        x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
        y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))
        Z = np.zeros(xx.shape)
        for i in range(len(class_labels)):
            Z = np.zeros(xx.shape)
            for j in range(xx.shape[0]):
                for k in range(xx.shape[1]):
                    sample_point = np.array([xx[j, k], yy[j, k]])
                    decision_value = 0
                    w, bias, a, sv_x, sv_y, kernel_and_params = classifiers[i]
                    for a1, sv_y1, sv1 in zip(a, sv_y, sv_x):
                        decision_value += a1 * sv_y1 * polynomial_kernel(sample_point, sv1, C=kernel_and_params[1][0], d=kernel_and_params[1][1])
                    decision_value += bias
                    Z[j, k] = decision_value
            plt.contour(xx, yy, Z, levels=[0], colors='r')

    if trainset:
        plt.title('Data Points')
    else:
        plt.title('Data Points on Test Set')

    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.legend()
    plt.xlim(np.min(X_train[:, 0]) - 1, np.max(X_train[:, 0]) + 1)
    plt.ylim(np.min(X_train[:, 1]) - 1, np.max(X_train[:, 1]) + 1)
    plt.savefig(save_path_scr)
    plt.show()

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    accuracies = []
from sklearn.metrics import accuracy_score
for degree in range(1, 11):
    print(f"Training with polynomial degree {degree}")
    predictions, kernel_and_params, w_catch, b_catch, classifiers = multiclass_svm(
        X_reduced_train, X_reduced_test, y_train, C=1.0, kernel_type='polynomial', poly_params=(1.0, degree)
    )
    accuracy = accuracy_score(y_test, predictions)
    accuracies.append(accuracy)
    print(f"Degree: {degree}, Accuracy: {accuracy}")

    visualize_multiclass_classification1(X_reduced_train, y_train, 'polynomial', True, classifiers, np.unique(y_train), w_catch, b_catch , save_path_scr = f'polynum_scratch_degree{degree}')

#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
import imageio

images_scr = []
for degree in range(1, 11):
    images_scr.append(imageio.imread(f'/content/polynum_scratch_degree{degree}.png'))
imageio.mimsave('svm_poly_scratch_kernels.gif', images_scr, duration=1)
print("GIF saved as 'svm_poly_scratch_kernels.gif'")
#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
accuracy_scr = []
for degree , acc in results:
  accuracy_scr.append(acc)
#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
# comparing results
for i in range(10):
  print(f'accuracy in degree{i+1}:\n sklearn_model:{accuracy_scr[i]} and our model:{accuracies[i]}')

"""## Comments"""

'''
_, ax = plt.subplots()
scatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)
ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])
_ = ax.legend(
    scatter.legend_elements()[0], iris.target_names, loc="lower right", title="Classes"
)
'''

'''
def plot_svm_decision_boundary(clf, X, y):
    # Create a mesh to plot the decision boundary
    h = .02  # step size in the mesh
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    # Plot the decision boundary
    plt.subplot(1, 1, 1)
    xy = np.vstack([xx.ravel(), yy.ravel()]).T
    Z = clf.decision_function(xy)

    # Check the shape of the decision function output
    if Z.ndim > 1:
        # Multi-class case: Take the maximum value across the decision functions
        Z = Z.max(axis=1)

    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)

    # Plot the training points
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')

    # Plot the support vectors
    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,
                facecolors='none', edgecolors='k', linewidths=1.5)

    # Plot the decision boundary and margins
    ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()

    xx = np.linspace(xlim[0], xlim[1], 30)
    yy = np.linspace(ylim[0], ylim[1], 30)
    YY, XX = np.meshgrid(yy, xx)
    xy = np.vstack([XX.ravel(), YY.ravel()]).T
    Z = clf.decision_function(xy)

    if Z.ndim > 1:
        # Multi-class case: Take the maximum value across the decision functions
        Z = Z.max(axis=1)

    Z = Z.reshape(XX.shape)

    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,
               linestyles=['--', '-', '--'])

    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title(f'SVM with {clf.kernel.capitalize()} Kernel')
    plt.show()

# Example: clf = SVC(kernel='rbf').fit(X_svc_trn, y_svc_trn)
'''

#plot_svm_decision_boundary(clf, X_svc_tst, y_svc_tst)

def make_meshgrid(x, y, h=.02):
    x_min, x_max = x.min() - 1, x.max() + 1
    y_min, y_max = y.min() - 1, y.max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    return xx, yy

def plot_contours(ax, clf, xx, yy, **params):
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    out = ax.contourf(xx, yy, Z, **params)
    return out

# Create a new figure and axis
fig, ax = plt.subplots()



# Assuming X_svc_trn is a numpy array, if it's a DataFrame, convert it to numpy array
X_svc_trn_array = X_svc_trn.to_numpy() if isinstance(X_svc_trn, pd.DataFrame) else X_svc_trn



# Extract the first and second columns from X_svc_trn_array
X0, X1 = X_svc_trn_array[:, 0], X_svc_trn_array[:, 1]

# Create meshgrid
xx, yy = make_meshgrid(X0, X1)

#clf = SVC(kernel='linear', C=0.5, random_state=random_state)

# Plot contours
plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)

# Scatter plot of training points
ax.scatter(X0, X1, c=y_svc_trn, cmap=plt.cm.coolwarm, s=20, edgecolors='k')
ax.set_xlim(xx.min(), xx.max())
ax.set_ylim(yy.min(), yy.max())
ax.set_xlabel('Feature 1')
ax.set_ylabel('Feature 2')
ax.set_xticks(())
ax.set_yticks(())
ax.set_title('SVM on IRIS Dataset')

# Show the plot
plt.show()

def plot_svm_decision_boundary(clf, X, y):
    # Create a mesh to plot the decision boundary
    h = .02  # step size in the mesh
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    # Plot the decision boundary
    plt.subplot(1, 1, 1)
    xy = np.vstack([xx.ravel(), yy.ravel()]).T
    Z = clf.decision_function(xy)

    # Check the shape of the decision function output
    if Z.ndim > 1:
        # Multi-class case: Take the maximum value across the decision functions
        Z = Z.max(axis=1)

    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)

    # Plot the training points
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')

    # Plot the support vectors
    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,
                facecolors='none', edgecolors='k', linewidths=1.5)

    # Plot the decision boundary and margins
    ax = plt.gca()
    xlim = ax.get_xlim()
    ylim = ax.get_ylim()

    xx = np.linspace(xlim[0], xlim[1], 30)
    yy = np.linspace(ylim[0], ylim[1], 30)
    YY, XX = np.meshgrid(yy, xx)
    xy = np.vstack([XX.ravel(), YY.ravel()]).T
    Z = clf.decision_function(xy)

    if Z.ndim > 1:
        # Multi-class case: Take the maximum value across the decision functions
        Z = Z.max(axis=1)

    Z = Z.reshape(XX.shape)

    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,
               linestyles=['--', '-', '--'])

    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title(f'SVM with {clf.kernel.capitalize()} Kernel')
    plt.show()

# Example: clf = SVC(kernel='rbf').fit(X_svc_trn, y_svc_trn)
#clf = SVC(kernel='rbf').fit(X_svc_trn, y_svc_trn)
plot_svm_decision_boundary(clf, X_svc_tst, y_svc_tst)

'''
data1 = pd.DataFrame(data= np.c_[iris['data'], iris['target']],
                     columns= iris['feature_names'] + ['target'])
data1
data1.head()
data1.info()
data1.describe()
'''

def plot_decision_boundary(svm_model, X, y, title):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                         np.arange(y_min, y_max, 0.01))

    Z = svm_model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(5, 5))
    plt.contourf(xx, yy, Z, alpha=0.1)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')
    plt.title(title)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')

plot_decision_boundary(clf, X_svc_trn, y_svc_trn, 'Polynomial Kernel SVM (Degree 1)')
#plot_decision_boundary(svm_poly2, X, y, 'Polynomial Kernel SVM (Degree 2)')
plt.show()

