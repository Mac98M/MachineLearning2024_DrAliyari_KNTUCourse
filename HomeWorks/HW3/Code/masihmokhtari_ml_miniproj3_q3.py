# -*- coding: utf-8 -*-
"""masihmokhtari_ml_miniproj3_Q3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yoCMqXnOiOpUPHhJt4Y-9fSK5ZLIJg0f

_Machine Learning Dr.Aliyari_


**Masih Mokhtari**

**40211454**
****
_mini project $3$_

$Q3$

# Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# At first we disable all warnings.
import warnings
warnings.filterwarnings("ignore")

# Before we start, we have to import libraries we need.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import accuracy_score , confusion_matrix
from sklearn.manifold import TSNE

from sklearn import datasets
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report

import pickle
from scipy import stats
import tensorflow as tf
from pylab import rcParams

from keras.models import Model, load_model
from keras.layers import Input, Dense
from keras.callbacks import ModelCheckpoint, TensorBoard
from keras import regularizers
import h5py

# %matplotlib inline

sns.set(style='whitegrid', palette='muted', font_scale=1)

rcParams['figure.figsize'] = 8, 5

RANDOM_SEED = 54
LABELS = ["Normal", "Fraud"]


Random_state = 54

"""# Download Dataset"""

!pip install --upgrade --no-cach-dir gdown
! gdown

df = pd.read_csv()

!pip install --upgrade --no-cach-dir gdown
! gdown 1CdNMF_D8enArP5DwmtffkhFbBxtAxC-p
# https://drive.google.com/file/d/1CdNMF_D8enArP5DwmtffkhFbBxtAxC-p/view?usp=sharing

"""# Load Dataset"""

df = pd.read_csv('creditcard.csv')

df

# Print some information about the dataframe
df.info()
print("\n")
# Print some statistical information about the dataframe
df.describe()

df.head()

# Checking missing values
missingvalues = df.isnull().sum().sum()
if missingvalues==0 :
  print('This dataset has no missong values')
else:
  print(f'This dataset has {missingvalues} missong values !')

# Assuming df is your DataFrame and LABELS is defined
count_classes = pd.value_counts(df['Class'], sort=True)
percent_classes = count_classes / count_classes.sum() * 100

# Define custom colors for the bars
colors = ['g', 'r']

# Plotting
percent_classes.plot(kind='bar', color=colors, rot=0)
plt.title("Fraud Class Histogram")
plt.xticks(range(len(LABELS)), LABELS)
plt.xlabel("Class")
plt.ylabel("Percentage")

# Add percentage labels on top of the bars
for index, value in enumerate(percent_classes):
    plt.text(index, value, f'{value:.2f}%', ha='center', va='bottom')

plt.show()

column = 'Class'
num_unique_values = df[column].nunique()
value_counts = df[column].value_counts()
print(f'The number of unique values in the column "{column}" is {num_unique_values}')
print('\n')
print(value_counts)
print('\n')
#y_train.shape
#print(y_train)

frauds = df[df.Class == 1]
normal = df[df.Class == 0]
print(f'Calss Normal shape : {normal.shape}\nCalss Frauds shape : {frauds.shape}')

frauds.Amount.describe()

normal.Amount.describe()

f, (ax1, ax2) = plt.subplots(2, 1, sharex=False, figsize = (7,7))
f.suptitle('Amount per transaction by class')

bins = 50

ax1.hist(frauds.Amount, bins = bins)
ax1.set_title('Fraud')

ax2.hist(normal.Amount, bins = bins)
ax2.set_title('Normal')

plt.xlabel('Amount ($)')
plt.ylabel('Number of Transactions')
plt.yscale('log')
plt.show()

f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize = (7,7))
f.suptitle('Time of transaction vs Amount by class')

ax1.scatter(frauds.Time, frauds.Amount)
ax1.set_title('Fraud')

ax2.scatter(normal.Time, normal.Amount)
ax2.set_title('Normal')

plt.xlabel('Time (in Seconds)')
plt.ylabel('Amount')
plt.show()

dir()
# Calculate and print the correlation matrix
corr = df.corr()

rounded_matrix_1 = np.round(corr, 3)
print(f'Cvariance Matrix of features is : \n\n {corr}\n\n')
print(f'Cvariance Matrix of features is : \n\n {rounded_matrix_1}\n\n')

plt.figure(figsize = (40, 40))
sns.heatmap(corr, xticklabels = corr.columns.values, yticklabels = corr.columns.values, cmap = "BuPu", vmin = -1, vmax = 1, annot = True)
plt.title("Correlation Heatmap")
plt.show()

# Plot histograms for numeric columns
df.hist(figsize=(40, 40))
plt.tight_layout()
plt.show()

# Display descriptive statistics
print("Descriptive Statistics:")
print(df.describe())

# Display the first five rows
print("First Five Rows:")
print(df.head())

# Display general information about the DataFrame
print("DataFrame Info:")
print(df.info())

# Print the column names
print("Column Names:")
print(df.columns)

# Print the shape of the DataFrame
print("DataFrame Shape:")
print(df.shape)

"""# Pre-Processing"""

df.drop(['Time'], axis = 1, inplace = True)

sc = StandardScaler()
df['Amount'] = sc.fit_transform(df[['Amount']])

X = df.drop(['Class'], axis = 1)
y = df['Class']
X.shape, y.shape

X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state= Random_state)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state= Random_state) # 0.25 * 0.8 = 0.2

# Calculate percentages
train_percent = len(X_train) / len(X) * 100
val_percent = len(X_val) / len(X) * 100
test_percent = len(X_test) / len(X) * 100

# Display the shapes and percentages
shapes = {
    "X_train_shape": X_train.shape,
    "X_val_shape": X_val.shape,
    "X_test_shape": X_test.shape,
    "y_train_shape": y_train.shape,
    "y_val_shape": y_val.shape,
    "y_test_shape": y_test.shape
}

percentages = {
    "train_percent": train_percent,
    "val_percent": val_percent,
    "test_percent": test_percent
}
shapes, percentages

"""# SMOTE"""

from imblearn.over_sampling import SMOTE
smote = SMOTE(sampling_strategy='minority', random_state = Random_state)
X_train, y_train = smote.fit_resample(X_train, y_train)

import matplotlib.pyplot as plt
import seaborn as sns
class_counts = y_train.value_counts()

# Set the style
plt.style.use('ggplot')

# Create a color palette
colors = sns.color_palette("coolwarm", len(class_counts))

# Plot
plt.figure(figsize=(10, 7))
bar_plot = class_counts.plot(kind='bar', color=colors, edgecolor='black')
plt.title('Training Class Distribution after Applying SMOTE', fontsize=16, fontweight='bold')
plt.xlabel('Class', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.xticks(rotation=0, fontsize=12)
plt.yticks(fontsize=12)
plt.grid(True, linestyle='--', linewidth=0.7, alpha=0.7)

# Add labels on the bars
for p in bar_plot.patches:
    bar_plot.annotate(format(p.get_height(), '.0f'),
                      (p.get_x() + p.get_width() / 2., p.get_height()),
                      ha = 'center', va = 'center',
                      xytext = (0, 9),
                      textcoords = 'offset points',
                      fontsize=12)

plt.show()

from tensorflow.keras.utils import to_categorical
y_train = to_categorical(y_train, num_classes=2)
y_val = to_categorical(y_val, num_classes=2)
y_test = to_categorical(y_test, num_classes=2)

"""# Part 3

**Noise and Denoising AutoEncoder**
"""

noise_factor = 0.2
X_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)
X_val_noisy = X_val + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_val.shape)
X_test_noisy = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.callbacks import ModelCheckpoint

# Build the denoising autoencoder
input_dim = X_train.shape[1]

input_layer = Input(shape=(input_dim,))
encoded = Dense(22, activation='relu')(input_layer)
encoded = Dense(15, activation='relu')(encoded)
encoded = Dense(10, activation='relu')(encoded)
decoded = Dense(15, activation='relu')(encoded)
decoded = Dense(22, activation='relu')(decoded)
output_layer = Dense(input_dim, activation='sigmoid')(decoded)

autoencoder = Model(input_layer, output_layer)
autoencoder.compile(optimizer='adam', loss='mse')

# Define checkpoint callback to save the best autoencoder model
autoencoder_checkpoint = ModelCheckpoint('best_autoencoder.h5', monitor='val_loss', save_best_only=True, mode='min')

# Train the autoencoder
autoencoder.fit(X_train_noisy,
                X_train,
                epochs=50,
                batch_size=256,
                shuffle=True,
                validation_data=(X_val, X_val),
                callbacks=[autoencoder_checkpoint],
                verbose = 1)

# Load the best autoencoder model
autoencoder.load_weights('best_autoencoder.h5')
# Denoise the training, validation, and test sets
X_train_denoised = autoencoder.predict(X_train_noisy)
X_val_denoised = autoencoder.predict(X_val_noisy)
X_test_denoised = autoencoder.predict(X_test_noisy)

X_train_denoised.shape, y_train.shape, y_val.shape

# Build the classification model
classifier_input = Input(shape=(input_dim,))
x = Dense(22, activation='relu')(classifier_input)
x = Dense(15, activation='relu')(x)
x = Dense(10, activation='relu')(x)
x = Dense(5, activation='relu')(x)
x = Dense(2, activation='softmax')(x)

classifier = Model(classifier_input, x)
classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Define checkpoint callback to save the best classifier model
classifier_checkpoint = ModelCheckpoint('best_classifier.h5', monitor='val_loss', save_best_only=True, mode='min')

# Train the classifier
classifier.fit(X_train_denoised,
               y_train,
               epochs=50,
               batch_size=256,
               shuffle = True,
               validation_data=(X_val_denoised, y_val),
               callbacks=[classifier_checkpoint],
               verbose = 1)

"""# Part 4"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score
import numpy as np

# Load the best classifier model
classifier.load_weights('best_classifier.h5')

# Predict on the denoised validation set
y_val_pred = classifier.predict(X_val_denoised)
y_val_pred_classes = np.argmax(y_val_pred, axis=1)
y_val_true_classes = np.argmax(y_val, axis=1)

# Calculate metrics for the validation set
conf_matrix = confusion_matrix(y_val_true_classes, y_val_pred_classes)
accuracy = accuracy_score(y_val_true_classes, y_val_pred_classes)
f1 = f1_score(y_val_true_classes, y_val_pred_classes, average='weighted')
recall = recall_score(y_val_true_classes, y_val_pred_classes, average='weighted')
precision = precision_score(y_val_true_classes, y_val_pred_classes, average='weighted')

# Print metrics
print(f'Validation Set Metrics:')
print(f'Accuracy   : {accuracy * 100:.2f}%')
print(f'F1 Score   : {f1:.2f}')
print(f'Recall     : {recall:.2f}')
print(f'Precision  : {precision:.2f}')

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
            annot_kws={"size": 14}, linewidths=.5, linecolor='black')
plt.title('Confusion Matrix', fontsize=16)
plt.xlabel('Predicted Class', fontsize=14)
plt.ylabel('Actual Class', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.show()

from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load the best classifier model
classifier.load_weights('best_classifier.h5')

# Predict on the denoised test set
y_test_pred = classifier.predict(X_test_denoised)
y_test_pred_classes = np.argmax(y_test_pred, axis=1)
y_test_true_classes = np.argmax(y_test, axis=1)

# Calculate metrics
accuracy = accuracy_score(y_test_true_classes, y_test_pred_classes)
precision, recall, f1, _ = precision_recall_fscore_support(y_test_true_classes, y_test_pred_classes, average='weighted')

# Print metrics
print(f'Accuracy: {accuracy*100:.2f}%')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1:.2f}')

# Print metrics
print(f'Validation Set Metrics:')
print(f'Accuracy   : {accuracy * 100:.2f}%')
print(f'F1 Score   : {f1:.2f}')
print(f'Recall     : {recall:.2f}')
print(f'Precision  : {precision:.2f}')

# Plot confusion matrix
plt.figure(figsize=(7, 5))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
            annot_kws={"size": 14}, linewidths=.5, linecolor='black')
plt.title('Confusion Matrix', fontsize=16)
plt.xlabel('Predicted Class', fontsize=14)
plt.ylabel('Actual Class', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.tight_layout()
plt.show()

print(classification_report(y_test_true_classes, y_test_pred_classes))



"""# Part 5"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import recall_score, accuracy_score

# Generate thresholds from 0 to 1 with step 0.05
thresholds = np.arange(0.0, 1.05, 0.05)

# Initialize lists to store recall and accuracy values
recalls = []
accuracies = []

# Calculate recall and accuracy for each threshold
for threshold in thresholds:
    # Predict classes based on the threshold
    y_pred = (y_test_pred[:, 1] >= threshold).astype(int)

    # Calculate recall and accuracy
    recall = recall_score(np.argmax(y_test, axis=1), y_pred)
    accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred)

    # Append to lists
    recalls.append(recall)
    accuracies.append(accuracy)

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(thresholds, recalls, label='Recall', marker='o')
plt.plot(thresholds, accuracies, label='Accuracy', marker='o')
plt.xlabel('Threshold (0 to 1)')
plt.ylabel('Score')
plt.title('Recall and Accuracy vs. Threshold')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

y_train = to_categorical(y_train, num_classes=2)
y_val = to_categorical(y_val, num_classes=2)
y_test = to_categorical(y_test, num_classes=2)

X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=24)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=24) # 0.25 * 0.8 = 0.2

y_train = to_categorical(y_train, num_classes=2)
y_val = to_categorical(y_val, num_classes=2)
y_test = to_categorical(y_test, num_classes=2)

sampling_strategies = [0.1, 0.25, 0.5, 0.75]

results = []

for strategy in sampling_strategies:
    # Apply SMOTE with the current sampling strategy
    smote = SMOTE(sampling_strategy=strategy, random_state=24)
    X_train_res, y_train_res = smote.fit_resample(X_train, np.argmax(y_train, axis=1))
    y_train_res = to_categorical(y_train_res, num_classes=2)

    print(f'Resampled dataset shape with strategy {strategy}:', X_train_res.shape, y_train_res.shape)

    # Adding Gaussian noise to the data
    def add_noise(data, noise_factor=0.2):
        noisy_data = data + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=data.shape)
        noisy_data = np.clip(noisy_data, 0., 1.)
        return noisy_data

    X_train_noisy = add_noise(X_train_res)
    X_val_noisy = add_noise(X_val)

    # Define the autoencoder model
    input_dim = X_train_res.shape[1]
    encoding_dim = 10

    input_layer = Input(shape=(input_dim,))
    encoder = Dense(encoding_dim, activation="relu")(input_layer)
    encoder = Dense(22, activation="relu")(encoder)
    encoder = Dense(15, activation="relu")(encoder)
    encoder = Dense(encoding_dim, activation="relu")(encoder)
    encoder = Dense(15, activation="relu")(encoder)
    encoder = Dense(22, activation="relu")(encoder)
    decoder = Dense(input_dim, activation='sigmoid')(encoder)

    autoencoder = Model(inputs=input_layer, outputs=decoder)
    autoencoder.compile(optimizer='adam', loss='mean_squared_error')

    # Train the autoencoder
    autoencoder.fit(X_train_noisy, X_train_res,
                    epochs=20,
                    batch_size=256,
                    shuffle=True,
                    validation_data=(X_val_noisy, X_val),
                    verbose=1)

    # Denoise the data
    X_train_denoised = autoencoder.predict(X_train_noisy)
    X_valid_denoised = autoencoder.predict(X_val_noisy)

    # Define the classifier model
    classifier_input = Input(shape=(input_dim,))
    classifier_layer = Dense(encoding_dim, activation="relu")(classifier_input)
    classifier_layer = Dense(22, activation="relu")(classifier_layer)
    classifier_layer = Dense(15, activation="relu")(classifier_layer)
    classifier_layer = Dense(10, activation="relu")(classifier_layer)
    classifier_layer = Dense(5, activation="relu")(classifier_layer)
    classifier_layer = Dense(2, activation='softmax')(classifier_layer)  # Two neurons with softmax activation

    classifier = Model(inputs=classifier_input, outputs=classifier_layer)
    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Categorical crossentropy loss

    # Train the classifier
    classifier.fit(X_train_denoised, y_train_res,
                   epochs=20,
                   batch_size=256,
                   shuffle=True,
                   validation_data=(X_val_denoised, y_val),
                   verbose=1)

    # Evaluate the classifier on the test set
    X_test_noisy = add_noise(X_test)
    X_test_denoised = autoencoder.predict(X_test_noisy)

    # Predict probabilities
    y_pred_prob = classifier.predict(X_test_denoised)

    # Define a fixed threshold for classification
    threshold = 0.5
    y_pred = (y_pred_prob[:, 1] >= threshold).astype(int)

    # Calculate recall and accuracy
    recall = recall_score(np.argmax(y_test, axis=1), y_pred)
    accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred)
    results.append({'SMOTE Threshold': strategy, 'Recall Rate': recall, 'Accuracy': accuracy})

# Create a DataFrame for the results
results_df = pd.DataFrame(results)

# Display the results
print(results_df)

# Plot recall and accuracy against SMOTE sampling strategies
plt.figure(figsize=(10, 6))
plt.plot(results_df['SMOTE Threshold'], results_df['Recall Rate'], label='Recall')
plt.plot(results_df['SMOTE Threshold'], results_df['Accuracy'], label='Accuracy')
plt.xlabel('SMOTE Sampling Strategy')
plt.ylabel('Metrics')
plt.title('Recall & Accuracy vs. SMOTE Sampling Strategy')
plt.legend()
plt.show()

"""# Part 6"""

import numpy as np
import pandas as pd
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import recall_score, accuracy_score, confusion_matrix, f1_score, precision_score
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv('creditcard.csv')

# Drop 'TIME' and normalize 'AMOUNT'
df = df.drop(columns=['Time'])
scaler = StandardScaler()
df['Amount'] = scaler.fit_transform(df[['Amount']])

# Split features and target
X = df.drop(columns=['Class'])
y = df['Class']

# Split into train, validation, and test sets (60%, 20%, 20%)
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=24)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=24)  # 0.25 * 0.8 = 0.2

# Convert labels to categorical format
y_train = to_categorical(y_train, num_classes=2)
y_val = to_categorical(y_val, num_classes=2)
y_test = to_categorical(y_test, num_classes=2)

# Add Gaussian noise to the data
def add_noise(data, noise_factor=0.2):
    noisy_data = data + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=data.shape)
    noisy_data = np.clip(noisy_data, 0., 1.)
    return noisy_data

X_train_noisy = add_noise(X_train)
X_val_noisy = add_noise(X_val)
X_test_noisy = add_noise(X_test)

# Define the classifier model
input_dim = X_train.shape[1]
classifier_input = Input(shape=(input_dim,))
classifier_layer = Dense(22, activation="relu")(classifier_input)
classifier_layer = Dense(15, activation="relu")(classifier_layer)
classifier_layer = Dense(10, activation="relu")(classifier_layer)
classifier_layer = Dense(5, activation="relu")(classifier_layer)
classifier_layer = Dense(2, activation='softmax')(classifier_layer)

classifier = Model(inputs=classifier_input, outputs=classifier_layer)
classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the classifier
classifier.fit(X_train_noisy, y_train,
               epochs=20,
               batch_size=256,
               shuffle=True,
               validation_data=(X_val_noisy, y_val),
               verbose=1)

# Predict probabilities on the test set
y_pred_prob = classifier.predict(X_test_noisy)

# Define a fixed threshold for classification
threshold = 0.5
y_pred = (y_pred_prob[:, 1] >= threshold).astype(int)

# Calculate recall, accuracy, F1 score, precision, and confusion matrix
recall = recall_score(np.argmax(y_test, axis=1), y_pred)
accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred)
f1 = f1_score(np.argmax(y_test, axis=1), y_pred)
precision = precision_score(np.argmax(y_test, axis=1), y_pred)
conf_matrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred)

# Print metrics
print(f'Accuracy: {accuracy*100:.2f}%')
print(f'F1 Score: {f1:.2f}')
print(f'Recall: {recall:.2f}')
print(f'Precision: {precision:.2f}')
print(classification_report(np.argmax(y_test, axis=1), y_pred))

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming conf_matrix is your confusion matrix variable
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, square=True)
plt.title('Confusion Matrix', fontsize=16)
plt.xlabel('Predicted Label', fontsize=14)
plt.ylabel('Actual Label', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.show()

# Sweep the threshold and calculate recall and accuracy
thresholds = np.arange(0.0, 1.05, 0.05)
recalls = []
accuracies = []

for threshold in thresholds:
    y_pred = (y_pred_prob[:, 1] >= threshold).astype(int)
    recall = recall_score(np.argmax(y_test, axis=1), y_pred)
    accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred)
    recalls.append(recall)
    accuracies.append(accuracy)

# Plot recall and accuracy against thresholds
plt.figure(figsize=(10, 6))
plt.plot(thresholds, recalls, label='Recall')
plt.plot(thresholds, accuracies, label='Accuracy')
plt.xlabel('Threshold (0~1)')
plt.ylabel('Recall & Accuracy')
plt.title('Recall & Accuracy vs. Threshold')
plt.legend()
plt.show()